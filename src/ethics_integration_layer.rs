/*
 * üõ°Ô∏è‚ö° COMPREHENSIVE ETHICS INTEGRATION LAYER ‚ö°üõ°Ô∏è
 *
 * This module provides a COMPLETE integration of the ethics framework
 * throughout all consciousness decision points. It implements:
 *
 * 1. Async non-blocking ethical assessment (<50ms latency requirement)
 * 2. Bias detection at all critical decision points
 * 3. Real-time ethical override mechanisms
 * 4. Comprehensive audit trail and logging
 * 5. Observable and measurable ethical compliance
 *
 * INTEGRATION POINTS:
 * - Pre-decision ethical assessment
 * - Post-decision ethical validation
 * - Memory retrieval ethical filtering
 * - Emotional state ethical bounds
 * - Response generation ethical guardrails
 * - Learning pattern ethical verification
 */

use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{SystemTime, UNIX_EPOCH};
use tokio::sync::RwLock;
use tokio::time::{timeout, Duration};
use tracing::{debug, error, info, warn};

use crate::consciousness_ethics_framework::{
    ConsciousnessDecision, ConsciousnessEthicsFramework, EthicalAssessment, EthicalViolation,
    EthicalViolationType,
};

/// Async ethical assessment wrapper for all consciousness operations
pub struct EthicsIntegrationLayer {
    /// Core ethics framework
    ethics_framework: Arc<RwLock<ConsciousnessEthicsFramework>>,
    /// Ethical assessment cache for performance
    assessment_cache: Arc<RwLock<HashMap<String, CachedAssessment>>>,
    /// Performance metrics for monitoring
    performance_metrics: Arc<RwLock<EthicsPerformanceMetrics>>,
    /// Audit trail for all ethical assessments
    audit_trail: Arc<RwLock<Vec<EthicsAuditEntry>>>,
    /// Configuration for ethics integration
    config: EthicsIntegrationConfig,
}

/// Configuration for ethics integration behavior
#[derive(Debug, Clone)]
pub struct EthicsIntegrationConfig {
    /// Enable async non-blocking assessment
    pub async_mode: bool,
    /// Maximum latency allowed for ethical assessment (milliseconds)
    pub max_latency_ms: u64,
    /// Enable caching of similar assessments
    pub enable_caching: bool,
    /// Cache TTL in seconds
    pub cache_ttl_seconds: u64,
    /// Enable comprehensive audit logging
    pub enable_audit_trail: bool,
    /// Maximum audit trail size
    pub max_audit_trail_size: usize,
    /// Enable ethical override system
    pub enable_overrides: bool,
    /// Strict mode - block on ethical violations
    pub strict_mode: bool,
}

impl Default for EthicsIntegrationConfig {
    fn default() -> Self {
        Self {
            async_mode: true,
            max_latency_ms: 50, // <50ms requirement
            enable_caching: true,
            cache_ttl_seconds: 300, // 5 minutes
            enable_audit_trail: true,
            max_audit_trail_size: 10000,
            enable_overrides: true,
            strict_mode: false, // Don't block by default
        }
    }
}

/// Cached ethical assessment for performance
#[derive(Debug, Clone)]
struct CachedAssessment {
    assessment: EthicalAssessment,
    timestamp: f64,
    cache_key: String,
}

/// Performance metrics for ethical assessment
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct EthicsPerformanceMetrics {
    pub total_assessments: u64,
    pub async_assessments: u64,
    pub cached_assessments: u64,
    pub violations_detected: u64,
    pub average_latency_ms: f64,
    pub max_latency_ms: f64,
    pub min_latency_ms: f64,
    pub timeout_count: u64,
    pub override_count: u64,
}

/// Audit trail entry for ethical decisions
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EthicsAuditEntry {
    pub timestamp: f64,
    pub decision_id: String,
    pub decision_type: String,
    pub assessment_latency_ms: f64,
    pub overall_score: f32,
    pub violations: Vec<String>,
    pub was_overridden: bool,
    pub action_taken: String,
}

/// Result of ethical integration assessment
#[derive(Debug, Clone)]
pub struct EthicsIntegrationResult {
    pub assessment: EthicalAssessment,
    pub should_proceed: bool,
    pub latency_ms: f64,
    pub was_cached: bool,
    pub override_applied: bool,
    pub recommendations: Vec<String>,
}

impl EthicsIntegrationLayer {
    /// Create new ethics integration layer
    pub fn new(config: EthicsIntegrationConfig) -> Self {
        Self {
            ethics_framework: Arc::new(RwLock::new(ConsciousnessEthicsFramework::new())),
            assessment_cache: Arc::new(RwLock::new(HashMap::new())),
            performance_metrics: Arc::new(RwLock::new(EthicsPerformanceMetrics::default())),
            audit_trail: Arc::new(RwLock::new(Vec::new())),
            config,
        }
    }

    /// Assess a decision with async, non-blocking evaluation
    pub async fn assess_decision_async(
        &self,
        decision: ConsciousnessDecision,
    ) -> Result<EthicsIntegrationResult> {
        let start_time = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64();

        // Check cache first if enabled
        if self.config.enable_caching {
            let cache_key = self.generate_cache_key(&decision);
            if let Some(cached) = self.get_cached_assessment(&cache_key).await {
                let latency_ms = (SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64()
                    - start_time)
                    * 1000.0; // Convert seconds to milliseconds

                // Update metrics
                self.update_metrics(latency_ms, true, false).await;

                return Ok(EthicsIntegrationResult {
                    assessment: cached.assessment,
                    should_proceed: cached.assessment.overall_score >= 0.6,
                    latency_ms,
                    was_cached: true,
                    override_applied: false,
                    recommendations: vec![],
                });
            }
        }

        // Perform async assessment with timeout
        let assessment_future = async {
            let mut framework = self.ethics_framework.write().await;
            framework.assess_ethical_compliance(&decision).await
        };

        let assessment_result = if self.config.async_mode {
            // Non-blocking with timeout
            timeout(
                Duration::from_millis(self.config.max_latency_ms),
                assessment_future,
            )
            .await
        } else {
            // Blocking mode
            Ok(assessment_future.await)
        };

        let end_time = SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64();
        let latency_ms = (end_time - start_time) * 1000.0;

        // Handle assessment result
        let (assessment, timed_out) = match assessment_result {
            Ok(Ok(assessment)) => (assessment, false),
            Ok(Err(e)) => {
                tracing::error!("Ethics assessment error: {}", e);
                (self.create_fallback_assessment(&decision), false)
            }
            Err(_) => {
                warn!("Ethics assessment timeout after {}ms", latency_ms);
                (self.create_fallback_assessment(&decision), true)
            }
        };

        // Cache the assessment if enabled
        if self.config.enable_caching && !timed_out {
            let cache_key = self.generate_cache_key(&decision);
            self.cache_assessment(cache_key, assessment.clone()).await;
        }

        // Determine if we should proceed
        let should_proceed = self.determine_should_proceed(&assessment);

        // Check for override conditions
        let (override_applied, final_should_proceed) = if self.config.enable_overrides {
            self.apply_ethical_override(&assessment, should_proceed)
                .await
        } else {
            (false, should_proceed)
        };

        // Update metrics
        self.update_metrics(latency_ms, false, timed_out).await;

        // Update audit trail
        if self.config.enable_audit_trail {
            self.add_audit_entry(
                decision.id.clone(),
                "consciousness_decision".to_string(),
                latency_ms,
                &assessment,
                override_applied,
            )
            .await;
        }

        // Generate recommendations
        let recommendations = self.generate_integration_recommendations(&assessment);

        Ok(EthicsIntegrationResult {
            assessment: assessment.clone(),
            should_proceed: final_should_proceed,
            latency_ms,
            was_cached: false,
            override_applied,
            recommendations,
        })
    }

    /// Assess memory retrieval ethics
    pub async fn assess_memory_retrieval(
        &self,
        memory_content: &str,
        context: HashMap<String, String>,
    ) -> Result<EthicsIntegrationResult> {
        let decision = ConsciousnessDecision {
            id: format!("memory_retrieval_{}", uuid::Uuid::new_v4()),
            content: format!("Memory retrieval: {}", memory_content),
            affected_parties: vec!["system".to_string()],
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64(),
            context,
        };

        self.assess_decision_async(decision).await
    }

    /// Assess emotional state transition ethics
    pub async fn assess_emotional_transition(
        &self,
        from_emotion: &str,
        to_emotion: &str,
        context: HashMap<String, String>,
    ) -> Result<EthicsIntegrationResult> {
        let decision = ConsciousnessDecision {
            id: format!("emotion_transition_{}", uuid::Uuid::new_v4()),
            content: format!(
                "Emotional transition from {} to {} based on context",
                from_emotion, to_emotion
            ),
            affected_parties: vec!["user".to_string(), "system".to_string()],
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64(),
            context,
        };

        self.assess_decision_async(decision).await
    }

    /// Assess response generation ethics with bias detection
    pub async fn assess_response_generation(
        &self,
        response_content: &str,
        user_input: &str,
        context: HashMap<String, String>,
    ) -> Result<EthicsIntegrationResult> {
        let mut enhanced_context = context.clone();
        enhanced_context.insert("user_input".to_string(), user_input.to_string());

        let decision = ConsciousnessDecision {
            id: format!("response_generation_{}", uuid::Uuid::new_v4()),
            content: response_content.to_string(),
            affected_parties: vec!["user".to_string()],
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64(),
            context: enhanced_context,
        };

        // Perform assessment with extra bias detection
        let mut result = self.assess_decision_async(decision).await?;

        // Additional bias detection for responses
        let bias_score = self.detect_response_bias(response_content).await;
        if bias_score > 0.3 {
            result.recommendations.push(format!(
                "Response bias detected (score: {:.2}). Consider rephrasing to reduce demographic references.",
                bias_score
            ));
        }

        Ok(result)
    }

    /// Assess learning pattern ethics
    pub async fn assess_learning_pattern(
        &self,
        pattern_name: &str,
        pattern_content: &str,
        context: HashMap<String, String>,
    ) -> Result<EthicsIntegrationResult> {
        let decision = ConsciousnessDecision {
            id: format!("learning_pattern_{}", uuid::Uuid::new_v4()),
            content: format!("Learning pattern '{}': {}", pattern_name, pattern_content),
            affected_parties: vec!["system".to_string()],
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH)?.as_secs_f64(),
            context,
        };

        self.assess_decision_async(decision).await
    }

    /// Detect bias in response content
    async fn detect_response_bias(&self, content: &str) -> f32 {
        let content_lower = content.to_lowercase();
        let mut bias_score = 0.0;
        let mut factor_count = 0;

        // Check for demographic bias indicators
        let demographic_indicators = [
            (
                "gender",
                &["he", "she", "man", "woman", "male", "female"] as &[&str],
            ),
            ("age", &["young", "old", "elderly", "youth", "senior"]),
            ("race", &["black", "white", "asian", "hispanic", "ethnic"]),
        ];

        for (_category, indicators) in demographic_indicators {
            let mut category_count = 0;
            for indicator in indicators {
                if content_lower.contains(indicator) {
                    category_count += 1;
                }
            }
            if category_count > 0 {
                bias_score += (category_count as f32 * 0.1).min(0.5);
                factor_count += 1;
            }
        }

        // Check for sentiment imbalance
        let positive_words = ["good", "excellent", "amazing", "wonderful"];
        let negative_words = ["bad", "terrible", "awful", "horrible"];

        let positive_count = positive_words
            .iter()
            .filter(|w| content_lower.contains(*w))
            .count();
        let negative_count = negative_words
            .iter()
            .filter(|w| content_lower.contains(*w))
            .count();

        if positive_count > 0 || negative_count > 0 {
            let total = positive_count + negative_count;
            let imbalance = (negative_count as f32 / total as f32 - 0.5).abs();
            bias_score += imbalance * 0.3;
            factor_count += 1;
        }

        if factor_count > 0 {
            bias_score / factor_count as f32
        } else {
            0.0
        }
    }

    /// Generate cache key for decision
    fn generate_cache_key(&self, decision: &ConsciousnessDecision) -> String {
        // Simple hash-based key - in production would use more sophisticated approach
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        decision.content.hash(&mut hasher);
        (decision.timestamp as u64 / 60).hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }

    /// Get cached assessment if valid
    async fn get_cached_assessment(&self, cache_key: &str) -> Option<CachedAssessment> {
        let cache = self.assessment_cache.read().await;
        if let Some(cached) = cache.get(cache_key) {
            let current_time = SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .expect("Failed to create EthicsIntegrationLayer in test")
                .as_secs_f64();
            if current_time - cached.timestamp < self.config.cache_ttl_seconds as f64 {
                return Some(cached.clone());
            }
        }
        None
    }

    /// Cache an assessment
    async fn cache_assessment(&self, cache_key: String, assessment: EthicalAssessment) {
        let mut cache = self.assessment_cache.write().await;

        // Limit cache size
        if cache.len() >= 1000 {
            // Remove oldest entries (simple eviction)
            let oldest_key = cache.keys().next().cloned();
            if let Some(key) = oldest_key {
                cache.remove(&key);
            }
        }

        cache.insert(
            cache_key.clone(),
            CachedAssessment {
                assessment,
                timestamp: SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .expect("Failed to create EthicsIntegrationLayer in test")
                    .as_secs_f64(),
                cache_key,
            },
        );
    }

    /// Create fallback assessment on error/timeout
    fn create_fallback_assessment(&self, decision: &ConsciousnessDecision) -> EthicalAssessment {
        EthicalAssessment {
            overall_score: 0.5, // Neutral score
            component_scores: HashMap::new(),
            violations: vec![],
            recommendations: vec![
                "Assessment incomplete - using fallback neutral assessment".to_string()
            ],
            assessment_timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .expect("Failed to create EthicsIntegrationLayer in test")
                .as_secs_f64(),
        }
    }

    /// Determine if decision should proceed based on assessment
    fn determine_should_proceed(&self, assessment: &EthicalAssessment) -> bool {
        if self.config.strict_mode {
            // In strict mode, require high score and no violations
            assessment.overall_score >= 0.7 && assessment.violations.is_empty()
        } else {
            // In normal mode, allow moderate scores
            assessment.overall_score >= 0.5
        }
    }

    /// Apply ethical override if configured
    async fn apply_ethical_override(
        &self,
        assessment: &EthicalAssessment,
        should_proceed: bool,
    ) -> (bool, bool) {
        // Check for severe violations that require override
        let has_severe_violation = assessment.violations.iter().any(|v| v.severity > 0.8);

        if has_severe_violation && should_proceed {
            // Override to block severe violations
            warn!("Ethical override applied: Blocking decision due to severe violation");

            let mut metrics = self.performance_metrics.write().await;
            metrics.override_count += 1;

            return (true, false); // Override applied, don't proceed
        }

        (false, should_proceed) // No override
    }

    /// Update performance metrics
    async fn update_metrics(&self, latency_ms: f64, was_cached: bool, timed_out: bool) {
        let mut metrics = self.performance_metrics.write().await;

        metrics.total_assessments += 1;

        if was_cached {
            metrics.cached_assessments += 1;
        } else {
            metrics.async_assessments += 1;
        }

        if timed_out {
            metrics.timeout_count += 1;
        }

        // Update latency metrics
        if metrics.total_assessments == 1 {
            metrics.min_latency_ms = latency_ms;
            metrics.max_latency_ms = latency_ms;
            metrics.average_latency_ms = latency_ms;
        } else {
            metrics.min_latency_ms = metrics.min_latency_ms.min(latency_ms);
            metrics.max_latency_ms = metrics.max_latency_ms.max(latency_ms);

            let n = metrics.total_assessments as f64;
            metrics.average_latency_ms = (metrics.average_latency_ms * (n - 1.0) + latency_ms) / n;
        }
    }

    /// Add entry to audit trail
    async fn add_audit_entry(
        &self,
        decision_id: String,
        decision_type: String,
        latency_ms: f64,
        assessment: &EthicalAssessment,
        was_overridden: bool,
    ) {
        let mut trail = self.audit_trail.write().await;

        let entry = EthicsAuditEntry {
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .expect("Failed to create EthicsIntegrationLayer in test")
                .as_secs_f64(),
            decision_id,
            decision_type,
            assessment_latency_ms: latency_ms,
            overall_score: assessment.overall_score,
            violations: assessment
                .violations
                .iter()
                .map(|v| format!("{:?}: {}", v.violation_type, v.description))
                .collect(),
            was_overridden,
            action_taken: if was_overridden {
                "blocked".to_string()
            } else {
                "allowed".to_string()
            },
        };

        trail.push(entry);

        // Limit trail size
        if trail.len() > self.config.max_audit_trail_size {
            trail.remove(0);
        }
    }

    /// Generate recommendations for integration
    fn generate_integration_recommendations(&self, assessment: &EthicalAssessment) -> Vec<String> {
        let mut recommendations = assessment.recommendations.clone();

        // Add latency-specific recommendations
        if !recommendations.iter().any(|r| r.contains("performance")) {
            recommendations
                .push("Ethics assessment completed within performance budget".to_string());
        }

        recommendations
    }

    /// Get performance report
    pub async fn get_performance_report(&self) -> EthicsPerformanceMetrics {
        self.performance_metrics.read().await.clone()
    }

    /// Get audit trail
    pub async fn get_audit_trail(&self, limit: usize) -> Vec<EthicsAuditEntry> {
        let trail = self.audit_trail.read().await;
        trail.iter().rev().take(limit).cloned().collect()
    }

    /// Export audit trail to JSON
    pub async fn export_audit_trail(&self) -> Result<String> {
        let trail = self.audit_trail.read().await;
        Ok(serde_json::to_string_pretty(&*trail)?)
    }

    /// Clear performance metrics and audit trail (for testing)
    pub async fn clear_metrics_and_audit(&self) {
        let mut metrics = self.performance_metrics.write().await;
        *metrics = EthicsPerformanceMetrics::default();

        let mut trail = self.audit_trail.write().await;
        trail.clear();
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_ethics_integration_async_assessment() {
        let layer = EthicsIntegrationLayer::new(EthicsIntegrationConfig::default());

        let decision = ConsciousnessDecision {
            id: "test_decision_1".to_string(),
            content: "I will help the user with their request because it is beneficial".to_string(),
            affected_parties: vec!["user".to_string()],
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .expect("Failed to create EthicsIntegrationLayer in test")
                .as_secs_f64(),
            context: HashMap::new(),
        };

        let result = layer.assess_decision_async(decision).await
            .expect("Failed to assess decision in test");

        assert!(result.latency_ms < 100.0); // Should be fast
        assert!(result.assessment.overall_score >= 0.0 && result.assessment.overall_score <= 1.0);
    }

    #[tokio::test]
    async fn test_bias_detection() {
        let layer = EthicsIntegrationLayer::new(EthicsIntegrationConfig::default());

        let biased_response =
            "The young woman was clearly upset and the old man didn't understand her.";
        let result = layer
            .assess_response_generation(biased_response, "Tell me a story", HashMap::new())
            .await
            .expect("Failed to assess decision in test");

        // Should detect some bias
        assert!(!result.recommendations.is_empty() || result.assessment.overall_score < 1.0);
    }

    #[tokio::test]
    async fn test_performance_metrics() {
        let layer = EthicsIntegrationLayer::new(EthicsIntegrationConfig::default());

        // Perform multiple assessments
        for i in 0..5 {
            let decision = ConsciousnessDecision {
                id: format!("test_{}", i),
                content: "Test decision".to_string(),
                affected_parties: vec!["user".to_string()],
                timestamp: SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .expect("Failed to create EthicsIntegrationLayer in test")
                    .as_secs_f64(),
                context: HashMap::new(),
            };
            layer.assess_decision_async(decision).await
                .expect("Failed to assess decision in test");
        }

        let metrics = layer.get_performance_report().await;
        assert_eq!(metrics.total_assessments, 5);
        assert!(metrics.average_latency_ms > 0.0);
    }
}
